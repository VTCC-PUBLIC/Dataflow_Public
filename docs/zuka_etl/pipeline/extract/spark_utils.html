<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.3" />
<title>zuka_etl.pipeline.extract.spark_utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>zuka_etl.pipeline.extract.spark_utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# -*- coding: utf-8 -*-

__author__ = &#39;phongphamhong&#39;

import pandas as pd
import pyspark.sql.types as types
from airflow.hooks.base_hook import BaseHook
from airflow.hooks.jdbc_hook import JdbcHook
from pyspark.sql.types import StructField, StructType

# !/usr/bin/python
#
# Copyright 11/9/18 Phong Pham Hong &lt;phongbro1805@gmail.com&gt;
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# # set enviroment in dev mode
from zuka_etl.log import logger
from zuka_etl.utils import generate_batches
from zuka_etl.utils import timeit


class SparkDfFromDriver(object):
    &#34;&#34;&#34;
        Using driver that setup by JAR and Spark Session
   
    &#34;&#34;&#34;

    @staticmethod
    def cast_columns(df, columns={}, lowercase_columns=True):
        &#34;&#34;&#34;
                :param df:
                :param columns:
                :param lowercase_columns:
                :return:
        &#34;&#34;&#34;
        cols = df.columns
        columns = {k.lower(): v.lower() for k, v in columns.items()}
        select = []
        for k in cols:
            l_k = k.lower()
            if columns.get(l_k):
                v = columns.get(l_k)
                select.append(&#34;CAST(`%s` as %s) as %s&#34; % (k, v, l_k if lowercase_columns else k))
            else:
                select.append(&#34;`%s` as %s&#34; % (k, l_k if lowercase_columns else k))
        logger.info(&#34;Columns will be casted: %s&#34; % columns)
        return df.selectExpr(*select)

    @staticmethod
    def from_jdbc(table: str, connection_id: str, spark_session=None,
                  auto_cast_field_id: bool = False,
                  lowercase_columns: bool = True,
                  cast_columns: dict = {}, options: dict = {}, **kwargs):
        &#34;&#34;&#34;
         create a dataframe with jdbc config
         
             params:
                    table: table or sql query: ex: (select * from a) s . You need to pass an 
                        alias like example
                    connection_id: defined from admin-connection airflow
                    kwargs:
                        params: param to replace on query. ex: select * from {table}, params will be: 
                        {&#39;table&#39;: &#39;database.test_table&#39;}
                    return:
            
        &#34;&#34;&#34;
        from zuka_etl.custom.spark_hook import SparkHook
        config = JdbcHook.get_connection(connection_id)
        session = spark_session if spark_session is not None else SparkHook().session
        table = table.strip().rstrip(&#39;;&#39;)
        if table.strip()[:1] != &#39;(&#39;:
            table = &#39;(%s) s&#39; % table

        df_load = (session.read.format(&#34;jdbc&#34;).option(&#34;url&#34;,
                                                      config.host)
                   .option(&#34;dbtable&#34;, table))
        if config.login:
            df_load = df_load.option(&#34;user&#34;, config.login)
        if config.password:
            df_load = df_load.option(&#34;password&#34;, config.password)
        df_load = df_load.option(&#34;driver&#34;, config.extra_dejson.get(&#34;extra__jdbc__drv_clsname&#34;)) \
            .option(&#34;isolationLevel&#34;, &#34;READ_UNCOMMITTED&#34;)
        if isinstance(options, dict) and options:
            for k, v in options.items():
                df_load = df_load.option(k, v)
        df = df_load.load()
        if auto_cast_field_id:
            col = df.columns
            sel = []
            for k in col:
                k_l = k.lower()
                if k_l == &#34;id&#34; or k_l.startswith(&#34;id_&#34;) or k_l.endswith(&#34;_id&#34;):
                    sel.append(&#34;CAST(%s as bigint) as %s&#34; % (k, k))
                else:
                    sel.append(k)
            return df.selectExpr(*sel)
        return SparkDfFromDriver.cast_columns(df, cast_columns, lowercase_columns)

    @staticmethod
    def from_elastic(index, query_string, connection_id, lowercase_columns=True,
                     cast_columns={}, **kwargs):
        &#34;&#34;&#34;
         Create a dataframe from elastichsearch

                index: elastichsearch index
                query q:  query string: q=dim_date:[1543536000 TO 1543536000] 
                    AND item_type:DEAL
                elastich_config: elastichsearch config name (you can find this name on 
                    Setting of project in DATA_SOURCE params to replace on query. 
                    ex: q=field_a:[{start} TO {stop}], params will be: {&#39;start&#39;: 1111111,&#39;stop&#39;: 1222222}
                return: pyspark.sql.dataframe.DataFrame
        
         &#34;&#34;&#34;
        from zuka_etl.custom.spark_hook import SparkHook
        config = BaseHook.get_connection(connection_id)
        table = SparkHook.replace_template(text=index, params=kwargs.get(&#39;params&#39;, {}))
        qr = &#39;%s?q=%s&#39; % (table, query_string)
        logger.info(
            &#39;[Spark] create dataframe with elastichsearch host: %s\nSource: \n%s\nFull query: %s\n&#39; % (
                config.host, index, qr))

        ct = SparkHook().sql_context.read.format(&#34;org.elasticsearch.spark.sql&#34;)
        cf = {
            &#34;es.nodes&#34;: config.host,
            &#34;es.port&#34;: config.port
        }
        if kwargs.get(&#39;select&#39;, &#39;&#39;):
            cf.update({
                &#39;es.read.field.include&#39;: kwargs.get(&#39;select&#39;, &#39;&#39;)
            })
        cf.update(kwargs.get(&#39;config&#39;, {}))
        cf.update(config.get(&#39;option&#39;, {}))
        for k, v in cf.items():
            logger.info(&#34;set conifg elastic: %s:%s&#34; % (k, v))
            ct = ct.option(k, v)
        df = ct.load(qr)
        return SparkDfFromDriver.cast_columns(df, cast_columns, lowercase_columns)

    @staticmethod
    def from_csv(file_path: str, spark_session=None,
                 is_clean_cols_name: bool = True,
                 lowercase_columns: bool = True,
                 cast_columns: dict = {}, transform_value: dict = {}, options_csv: dict = {}, output_query: str = &#34;&#34;):
        &#34;&#34;&#34;
        Import CSV file as Spark DataFrame
        Parameter:
            :param file_path: file path that will be imported
            :param spark_session:
            :param is_clean_cols_name: clean column name of csv file. Ex: &#34;Total Number -&gt; total_number&#34;
            :param cast_columns: convert data type of column. Ex: {&#34;number&#34;: &#34;int&#34;}
            :param transform_value: transform data of column. Ex: &#34;number&#34;: &#34;cast(number as int) as number&#34;
            :param options_csv: Options for Spark read csv
            :param output_query: If this param is set. Dataframe will be registered as table: temp_1 and you can pass query to get data from table: temp_1.
                                 Ex: &#34;select * from temp_1 where number &gt; 1&#34;
        :return:
        &#34;&#34;&#34;
        from zuka_etl.pipeline.load.spark_utils import SparkDfToDriver
        from zuka_etl.custom.spark_hook import SparkHook
        from zuka_etl.pipeline.transform.spark_utils import SparkTransCols
        from zuka_etl.helpers.sql import clean_col_name
        session = spark_session if spark_session is not None else SparkHook().session
        default_option = {&#34;delimiter&#34;: &#39;;&#39;,
                          &#34;header&#34;: True,
                          &#34;encoding&#34;: &#34;utf-8&#34;,
                          &#34;escape&#34;: &#39;&#34;&#39;,
                          &#34;multiLine&#34;: True}
        default_option.update(options_csv)
        df = session.read
        for k, v in default_option.items():
            df = df.option(k, v)
        df = df.csv(file_path)
        if is_clean_cols_name:
            for k in df.columns:
                n_col = clean_col_name(k)
                df = df.withColumnRenamed(k, n_col)
        if cast_columns or lowercase_columns:
            df = SparkDfToDriver.cast_columns(df, columns=cast_columns, lowercase_columns=lowercase_columns)
        if transform_value:
            df = SparkTransCols.trans_cols(df, transform_value=transform_value)
        if output_query:
            df.createOrReplaceTempView(&#34;temp_1&#34;)
            df = session.sql(output_query)
        return df


class SparkDfFromIterator(object):
    DEFAULT_BATCH_SIZE = 100000
    &#34;&#34;&#34;
    Import data from iterator to Spark Dataframe
    &#34;&#34;&#34;

    @staticmethod
    def correct_dtype_by_pandas(data):
        df = pd.DataFrame(data)
        t = df.dtypes
        rs = StructType()
        logger.info(&#34;Pandas type: %s&#34; % [&#34;%s:%s&#34; % (k, v) for k, v in df.dtypes.items()])
        for col, t in t.items():
            if t == &#34;int64&#34;:
                rs.add(StructField(col, types.LongType()))
            elif t == &#34;int&#34;:
                rs.add(StructField(col, types.IntegerType()))
            elif t == &#34;float64&#34;:
                rs.add(StructField(col, types.FloatType()))
            elif t == &#34;bool&#34;:
                rs.add(StructField(col, types.BooleanType()))
            elif t == &#34;datetime64&#34;:
                rs.add(StructField(col, types.DateType()))
            else:
                rs.add(StructField(col, types.StringType()))
        return rs

    @staticmethod
    @timeit
    def import_batch(data, session, schema):
        if schema is not None:
            df_temp = session.createDataFrame(data, schema)
        else:
            df_temp = session.createDataFrame(data)
        return df_temp

    @staticmethod
    def from_iterator(iter,
                      batch_size=DEFAULT_BATCH_SIZE,
                      transform_item=None,
                      spark_session=None,
                      schema=None,
                      auto_correct_schema=False,
                      **kwargs):
        &#34;&#34;&#34;
        Create dataframe with list of data: [{&#34;id&#34; : 1,&#34;name&#34;: &#34;phong&#34;},{&#34;id&#34; : 2,&#34;name&#34;: &#34;phong&#34;}]

            iter: list data
            batch_size: data will split into batches, this is config of size of each batch
            transform_item: transform function if you want to transform each value
            spark_session:
            schema: Spark Schema DF of Spark
            kwargs:
            return:
        
        &#34;&#34;&#34;
        from zuka_etl.custom.spark_hook import SparkHook
        f = generate_batches(iter, batch_size_limit=batch_size, callback_item=transform_item, remove_none=True)
        sp = spark_session if spark_session is not None else SparkHook().session
        df = None
        n = 0
        total = 0
        for batch in f:
            if auto_correct_schema is True and n == 0 and schema is None:
                schema = SparkDfFromIterator.correct_dtype_by_pandas(data=batch)
                logger.info(&#34;schema auto correct is: %s&#34; % schema)
            df_temp = SparkDfFromIterator.import_batch(batch, sp, schema)
            if n &lt;= 0:
                df = df_temp
            else:
                df = df.union(df_temp)
            total += len(batch)
            logger.info(&#34;Import to df: %s done!&#34; % total)
            n += 1
        return df

    @staticmethod
    def from_iterator_with_checkpoint(iter, checkpoint_func, return_func, batch_size=DEFAULT_BATCH_SIZE,
                                      transform_item=None,
                                      spark_session=None,
                                      schema=None,
                                      auto_correct_schema=False,
                                      **kwargs):
        &#34;&#34;&#34;
        Create dataframe with list of data: [{&#34;id&#34; : 1,&#34;name&#34;: &#34;phong&#34;},{&#34;id&#34; : 2,&#34;name&#34;: &#34;phong&#34;}]

            iter: list data
            checkpoint_func: function process a dataframe. Format: checkpoint_func(index_batch, df_batch)
            return_func: function output data: Format: return_func(spark_session)
            batch_size: data will split into batches, this is config of size of each batch
            transform_item: transform function if you want to transform each value
            spark_session:
            schema: Spark Schema DF of Spark
            kwargs:
            return:
        
        &#34;&#34;&#34;
        from zuka_etl.custom.spark_hook import SparkHook
        if callable(checkpoint_func) is False:
            raise ValueError(&#34;checkpoint_func must be function&#34;)
        f = generate_batches(iter, batch_size_limit=batch_size, callback_item=transform_item, remove_none=True)
        sp = spark_session if spark_session is not None else SparkHook().session
        n = 0
        total = 0
        for batch in f:
            if auto_correct_schema is True and n == 0 and schema is None:
                schema = SparkDfFromIterator.correct_dtype_by_pandas(data=batch)
                logger.info(&#34;schema auto correct is: %s&#34; % schema)
            df_temp = SparkDfFromIterator.import_batch(batch, sp, schema)
            logger.info(&#34;Process checkpoint from index: %s&#34; % n)
            checkpoint_func(n, df_temp)
            total += len(batch)
            logger.info(&#34;Import to df: %s done!&#34; % total)
            n += 1

        return return_func(sp)

    @staticmethod
    def from_iterator_with_checkpoint_hdfs(iter,
                                           write_function,
                                           return_function,
                                           batch_size=DEFAULT_BATCH_SIZE,
                                           transform_item=None,
                                           spark_session=None,
                                           schema=None, mode=&#34;append&#34;,
                                           auto_correct_schema=False,
                                           **kwargs):
        &#34;&#34;&#34;
        Create dataframe with list of data: [{&#34;id&#34; : 1,&#34;name&#34;: &#34;phong&#34;},{&#34;id&#34; : 2,&#34;name&#34;: &#34;phong&#34;}]

            iter: list data
            write_function: callable function for writing data: write_function(p): p.format(&#34;parquet&#34;).save(path hdfs)
            return_function: function for output data: Format: return_function(spark_session)
            batch_size: data will split into batches, this is config of size of each batch
            transform_item: transform function if you want to transform each value
            spark_session:
            schema: Spark Schema DF of Spark
            mode: append or replace targer ouput
            kwargs:
            return:
        
        &#34;&#34;&#34;
        if callable(write_function) is False:
            raise ValueError(&#34;write_function must be function&#34;)

        def checkpoint_func(index, df):
            c = df.write.mode(&#34;overwrite&#34; if (index == 0 and mode == &#34;replace&#34;) else &#34;append&#34;)
            return write_function(c)

        return SparkDfFromIterator.from_iterator_with_checkpoint(iter=iter,
                                                                 checkpoint_func=checkpoint_func,
                                                                 return_func=return_function,
                                                                 batch_size=batch_size,
                                                                 transform_item=transform_item,
                                                                 spark_session=spark_session,
                                                                 schema=schema,
                                                                 auto_correct_schema=auto_correct_schema,
                                                                 **kwargs
                                                                 )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver"><code class="flex name class">
<span>class <span class="ident">SparkDfFromDriver</span></span>
</code></dt>
<dd>
<div class="desc"><p>Using driver that setup by JAR and Spark Session</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SparkDfFromDriver(object):
    &#34;&#34;&#34;
        Using driver that setup by JAR and Spark Session
   
    &#34;&#34;&#34;

    @staticmethod
    def cast_columns(df, columns={}, lowercase_columns=True):
        &#34;&#34;&#34;
                :param df:
                :param columns:
                :param lowercase_columns:
                :return:
        &#34;&#34;&#34;
        cols = df.columns
        columns = {k.lower(): v.lower() for k, v in columns.items()}
        select = []
        for k in cols:
            l_k = k.lower()
            if columns.get(l_k):
                v = columns.get(l_k)
                select.append(&#34;CAST(`%s` as %s) as %s&#34; % (k, v, l_k if lowercase_columns else k))
            else:
                select.append(&#34;`%s` as %s&#34; % (k, l_k if lowercase_columns else k))
        logger.info(&#34;Columns will be casted: %s&#34; % columns)
        return df.selectExpr(*select)

    @staticmethod
    def from_jdbc(table: str, connection_id: str, spark_session=None,
                  auto_cast_field_id: bool = False,
                  lowercase_columns: bool = True,
                  cast_columns: dict = {}, options: dict = {}, **kwargs):
        &#34;&#34;&#34;
         create a dataframe with jdbc config
         
             params:
                    table: table or sql query: ex: (select * from a) s . You need to pass an 
                        alias like example
                    connection_id: defined from admin-connection airflow
                    kwargs:
                        params: param to replace on query. ex: select * from {table}, params will be: 
                        {&#39;table&#39;: &#39;database.test_table&#39;}
                    return:
            
        &#34;&#34;&#34;
        from zuka_etl.custom.spark_hook import SparkHook
        config = JdbcHook.get_connection(connection_id)
        session = spark_session if spark_session is not None else SparkHook().session
        table = table.strip().rstrip(&#39;;&#39;)
        if table.strip()[:1] != &#39;(&#39;:
            table = &#39;(%s) s&#39; % table

        df_load = (session.read.format(&#34;jdbc&#34;).option(&#34;url&#34;,
                                                      config.host)
                   .option(&#34;dbtable&#34;, table))
        if config.login:
            df_load = df_load.option(&#34;user&#34;, config.login)
        if config.password:
            df_load = df_load.option(&#34;password&#34;, config.password)
        df_load = df_load.option(&#34;driver&#34;, config.extra_dejson.get(&#34;extra__jdbc__drv_clsname&#34;)) \
            .option(&#34;isolationLevel&#34;, &#34;READ_UNCOMMITTED&#34;)
        if isinstance(options, dict) and options:
            for k, v in options.items():
                df_load = df_load.option(k, v)
        df = df_load.load()
        if auto_cast_field_id:
            col = df.columns
            sel = []
            for k in col:
                k_l = k.lower()
                if k_l == &#34;id&#34; or k_l.startswith(&#34;id_&#34;) or k_l.endswith(&#34;_id&#34;):
                    sel.append(&#34;CAST(%s as bigint) as %s&#34; % (k, k))
                else:
                    sel.append(k)
            return df.selectExpr(*sel)
        return SparkDfFromDriver.cast_columns(df, cast_columns, lowercase_columns)

    @staticmethod
    def from_elastic(index, query_string, connection_id, lowercase_columns=True,
                     cast_columns={}, **kwargs):
        &#34;&#34;&#34;
         Create a dataframe from elastichsearch

                index: elastichsearch index
                query q:  query string: q=dim_date:[1543536000 TO 1543536000] 
                    AND item_type:DEAL
                elastich_config: elastichsearch config name (you can find this name on 
                    Setting of project in DATA_SOURCE params to replace on query. 
                    ex: q=field_a:[{start} TO {stop}], params will be: {&#39;start&#39;: 1111111,&#39;stop&#39;: 1222222}
                return: pyspark.sql.dataframe.DataFrame
        
         &#34;&#34;&#34;
        from zuka_etl.custom.spark_hook import SparkHook
        config = BaseHook.get_connection(connection_id)
        table = SparkHook.replace_template(text=index, params=kwargs.get(&#39;params&#39;, {}))
        qr = &#39;%s?q=%s&#39; % (table, query_string)
        logger.info(
            &#39;[Spark] create dataframe with elastichsearch host: %s\nSource: \n%s\nFull query: %s\n&#39; % (
                config.host, index, qr))

        ct = SparkHook().sql_context.read.format(&#34;org.elasticsearch.spark.sql&#34;)
        cf = {
            &#34;es.nodes&#34;: config.host,
            &#34;es.port&#34;: config.port
        }
        if kwargs.get(&#39;select&#39;, &#39;&#39;):
            cf.update({
                &#39;es.read.field.include&#39;: kwargs.get(&#39;select&#39;, &#39;&#39;)
            })
        cf.update(kwargs.get(&#39;config&#39;, {}))
        cf.update(config.get(&#39;option&#39;, {}))
        for k, v in cf.items():
            logger.info(&#34;set conifg elastic: %s:%s&#34; % (k, v))
            ct = ct.option(k, v)
        df = ct.load(qr)
        return SparkDfFromDriver.cast_columns(df, cast_columns, lowercase_columns)

    @staticmethod
    def from_csv(file_path: str, spark_session=None,
                 is_clean_cols_name: bool = True,
                 lowercase_columns: bool = True,
                 cast_columns: dict = {}, transform_value: dict = {}, options_csv: dict = {}, output_query: str = &#34;&#34;):
        &#34;&#34;&#34;
        Import CSV file as Spark DataFrame
        Parameter:
            :param file_path: file path that will be imported
            :param spark_session:
            :param is_clean_cols_name: clean column name of csv file. Ex: &#34;Total Number -&gt; total_number&#34;
            :param cast_columns: convert data type of column. Ex: {&#34;number&#34;: &#34;int&#34;}
            :param transform_value: transform data of column. Ex: &#34;number&#34;: &#34;cast(number as int) as number&#34;
            :param options_csv: Options for Spark read csv
            :param output_query: If this param is set. Dataframe will be registered as table: temp_1 and you can pass query to get data from table: temp_1.
                                 Ex: &#34;select * from temp_1 where number &gt; 1&#34;
        :return:
        &#34;&#34;&#34;
        from zuka_etl.pipeline.load.spark_utils import SparkDfToDriver
        from zuka_etl.custom.spark_hook import SparkHook
        from zuka_etl.pipeline.transform.spark_utils import SparkTransCols
        from zuka_etl.helpers.sql import clean_col_name
        session = spark_session if spark_session is not None else SparkHook().session
        default_option = {&#34;delimiter&#34;: &#39;;&#39;,
                          &#34;header&#34;: True,
                          &#34;encoding&#34;: &#34;utf-8&#34;,
                          &#34;escape&#34;: &#39;&#34;&#39;,
                          &#34;multiLine&#34;: True}
        default_option.update(options_csv)
        df = session.read
        for k, v in default_option.items():
            df = df.option(k, v)
        df = df.csv(file_path)
        if is_clean_cols_name:
            for k in df.columns:
                n_col = clean_col_name(k)
                df = df.withColumnRenamed(k, n_col)
        if cast_columns or lowercase_columns:
            df = SparkDfToDriver.cast_columns(df, columns=cast_columns, lowercase_columns=lowercase_columns)
        if transform_value:
            df = SparkTransCols.trans_cols(df, transform_value=transform_value)
        if output_query:
            df.createOrReplaceTempView(&#34;temp_1&#34;)
            df = session.sql(output_query)
        return df</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver.cast_columns"><code class="name flex">
<span>def <span class="ident">cast_columns</span></span>(<span>df, columns={}, lowercase_columns=True)</span>
</code></dt>
<dd>
<div class="desc"><p>:param df:
:param columns:
:param lowercase_columns:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def cast_columns(df, columns={}, lowercase_columns=True):
    &#34;&#34;&#34;
            :param df:
            :param columns:
            :param lowercase_columns:
            :return:
    &#34;&#34;&#34;
    cols = df.columns
    columns = {k.lower(): v.lower() for k, v in columns.items()}
    select = []
    for k in cols:
        l_k = k.lower()
        if columns.get(l_k):
            v = columns.get(l_k)
            select.append(&#34;CAST(`%s` as %s) as %s&#34; % (k, v, l_k if lowercase_columns else k))
        else:
            select.append(&#34;`%s` as %s&#34; % (k, l_k if lowercase_columns else k))
    logger.info(&#34;Columns will be casted: %s&#34; % columns)
    return df.selectExpr(*select)</code></pre>
</details>
</dd>
<dt id="zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver.from_csv"><code class="name flex">
<span>def <span class="ident">from_csv</span></span>(<span>file_path: str, spark_session=None, is_clean_cols_name: bool = True, lowercase_columns: bool = True, cast_columns: dict = {}, transform_value: dict = {}, options_csv: dict = {}, output_query: str = '')</span>
</code></dt>
<dd>
<div class="desc"><p>Import CSV file as Spark DataFrame</p>
<h2 id="parameter">Parameter</h2>
<p>:param file_path: file path that will be imported
:param spark_session:
:param is_clean_cols_name: clean column name of csv file. Ex: "Total Number -&gt; total_number"
:param cast_columns: convert data type of column. Ex: {"number": "int"}
:param transform_value: transform data of column. Ex: "number": "cast(number as int) as number"
:param options_csv: Options for Spark read csv
:param output_query: If this param is set. Dataframe will be registered as table: temp_1 and you can pass query to get data from table: temp_1.
Ex: "select * from temp_1 where number &gt; 1"
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_csv(file_path: str, spark_session=None,
             is_clean_cols_name: bool = True,
             lowercase_columns: bool = True,
             cast_columns: dict = {}, transform_value: dict = {}, options_csv: dict = {}, output_query: str = &#34;&#34;):
    &#34;&#34;&#34;
    Import CSV file as Spark DataFrame
    Parameter:
        :param file_path: file path that will be imported
        :param spark_session:
        :param is_clean_cols_name: clean column name of csv file. Ex: &#34;Total Number -&gt; total_number&#34;
        :param cast_columns: convert data type of column. Ex: {&#34;number&#34;: &#34;int&#34;}
        :param transform_value: transform data of column. Ex: &#34;number&#34;: &#34;cast(number as int) as number&#34;
        :param options_csv: Options for Spark read csv
        :param output_query: If this param is set. Dataframe will be registered as table: temp_1 and you can pass query to get data from table: temp_1.
                             Ex: &#34;select * from temp_1 where number &gt; 1&#34;
    :return:
    &#34;&#34;&#34;
    from zuka_etl.pipeline.load.spark_utils import SparkDfToDriver
    from zuka_etl.custom.spark_hook import SparkHook
    from zuka_etl.pipeline.transform.spark_utils import SparkTransCols
    from zuka_etl.helpers.sql import clean_col_name
    session = spark_session if spark_session is not None else SparkHook().session
    default_option = {&#34;delimiter&#34;: &#39;;&#39;,
                      &#34;header&#34;: True,
                      &#34;encoding&#34;: &#34;utf-8&#34;,
                      &#34;escape&#34;: &#39;&#34;&#39;,
                      &#34;multiLine&#34;: True}
    default_option.update(options_csv)
    df = session.read
    for k, v in default_option.items():
        df = df.option(k, v)
    df = df.csv(file_path)
    if is_clean_cols_name:
        for k in df.columns:
            n_col = clean_col_name(k)
            df = df.withColumnRenamed(k, n_col)
    if cast_columns or lowercase_columns:
        df = SparkDfToDriver.cast_columns(df, columns=cast_columns, lowercase_columns=lowercase_columns)
    if transform_value:
        df = SparkTransCols.trans_cols(df, transform_value=transform_value)
    if output_query:
        df.createOrReplaceTempView(&#34;temp_1&#34;)
        df = session.sql(output_query)
    return df</code></pre>
</details>
</dd>
<dt id="zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver.from_elastic"><code class="name flex">
<span>def <span class="ident">from_elastic</span></span>(<span>index, query_string, connection_id, lowercase_columns=True, cast_columns={}, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a dataframe from elastichsearch</p>
<pre><code>   index: elastichsearch index
   query q:  query string: q=dim_date:[1543536000 TO 1543536000] 
       AND item_type:DEAL
   elastich_config: elastichsearch config name (you can find this name on 
       Setting of project in DATA_SOURCE params to replace on query. 
       ex: q=field_a:[{start} TO {stop}], params will be: {'start': 1111111,'stop': 1222222}
   return: pyspark.sql.dataframe.DataFrame
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_elastic(index, query_string, connection_id, lowercase_columns=True,
                 cast_columns={}, **kwargs):
    &#34;&#34;&#34;
     Create a dataframe from elastichsearch

            index: elastichsearch index
            query q:  query string: q=dim_date:[1543536000 TO 1543536000] 
                AND item_type:DEAL
            elastich_config: elastichsearch config name (you can find this name on 
                Setting of project in DATA_SOURCE params to replace on query. 
                ex: q=field_a:[{start} TO {stop}], params will be: {&#39;start&#39;: 1111111,&#39;stop&#39;: 1222222}
            return: pyspark.sql.dataframe.DataFrame
    
     &#34;&#34;&#34;
    from zuka_etl.custom.spark_hook import SparkHook
    config = BaseHook.get_connection(connection_id)
    table = SparkHook.replace_template(text=index, params=kwargs.get(&#39;params&#39;, {}))
    qr = &#39;%s?q=%s&#39; % (table, query_string)
    logger.info(
        &#39;[Spark] create dataframe with elastichsearch host: %s\nSource: \n%s\nFull query: %s\n&#39; % (
            config.host, index, qr))

    ct = SparkHook().sql_context.read.format(&#34;org.elasticsearch.spark.sql&#34;)
    cf = {
        &#34;es.nodes&#34;: config.host,
        &#34;es.port&#34;: config.port
    }
    if kwargs.get(&#39;select&#39;, &#39;&#39;):
        cf.update({
            &#39;es.read.field.include&#39;: kwargs.get(&#39;select&#39;, &#39;&#39;)
        })
    cf.update(kwargs.get(&#39;config&#39;, {}))
    cf.update(config.get(&#39;option&#39;, {}))
    for k, v in cf.items():
        logger.info(&#34;set conifg elastic: %s:%s&#34; % (k, v))
        ct = ct.option(k, v)
    df = ct.load(qr)
    return SparkDfFromDriver.cast_columns(df, cast_columns, lowercase_columns)</code></pre>
</details>
</dd>
<dt id="zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver.from_jdbc"><code class="name flex">
<span>def <span class="ident">from_jdbc</span></span>(<span>table: str, connection_id: str, spark_session=None, auto_cast_field_id: bool = False, lowercase_columns: bool = True, cast_columns: dict = {}, options: dict = {}, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>create a dataframe with jdbc config</p>
<pre><code>params:
       table: table or sql query: ex: (select * from a) s . You need to pass an 
           alias like example
       connection_id: defined from admin-connection airflow
       kwargs:
           params: param to replace on query. ex: select * from {table}, params will be: 
           {'table': 'database.test_table'}
       return:
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_jdbc(table: str, connection_id: str, spark_session=None,
              auto_cast_field_id: bool = False,
              lowercase_columns: bool = True,
              cast_columns: dict = {}, options: dict = {}, **kwargs):
    &#34;&#34;&#34;
     create a dataframe with jdbc config
     
         params:
                table: table or sql query: ex: (select * from a) s . You need to pass an 
                    alias like example
                connection_id: defined from admin-connection airflow
                kwargs:
                    params: param to replace on query. ex: select * from {table}, params will be: 
                    {&#39;table&#39;: &#39;database.test_table&#39;}
                return:
        
    &#34;&#34;&#34;
    from zuka_etl.custom.spark_hook import SparkHook
    config = JdbcHook.get_connection(connection_id)
    session = spark_session if spark_session is not None else SparkHook().session
    table = table.strip().rstrip(&#39;;&#39;)
    if table.strip()[:1] != &#39;(&#39;:
        table = &#39;(%s) s&#39; % table

    df_load = (session.read.format(&#34;jdbc&#34;).option(&#34;url&#34;,
                                                  config.host)
               .option(&#34;dbtable&#34;, table))
    if config.login:
        df_load = df_load.option(&#34;user&#34;, config.login)
    if config.password:
        df_load = df_load.option(&#34;password&#34;, config.password)
    df_load = df_load.option(&#34;driver&#34;, config.extra_dejson.get(&#34;extra__jdbc__drv_clsname&#34;)) \
        .option(&#34;isolationLevel&#34;, &#34;READ_UNCOMMITTED&#34;)
    if isinstance(options, dict) and options:
        for k, v in options.items():
            df_load = df_load.option(k, v)
    df = df_load.load()
    if auto_cast_field_id:
        col = df.columns
        sel = []
        for k in col:
            k_l = k.lower()
            if k_l == &#34;id&#34; or k_l.startswith(&#34;id_&#34;) or k_l.endswith(&#34;_id&#34;):
                sel.append(&#34;CAST(%s as bigint) as %s&#34; % (k, k))
            else:
                sel.append(k)
        return df.selectExpr(*sel)
    return SparkDfFromDriver.cast_columns(df, cast_columns, lowercase_columns)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator"><code class="flex name class">
<span>class <span class="ident">SparkDfFromIterator</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SparkDfFromIterator(object):
    DEFAULT_BATCH_SIZE = 100000
    &#34;&#34;&#34;
    Import data from iterator to Spark Dataframe
    &#34;&#34;&#34;

    @staticmethod
    def correct_dtype_by_pandas(data):
        df = pd.DataFrame(data)
        t = df.dtypes
        rs = StructType()
        logger.info(&#34;Pandas type: %s&#34; % [&#34;%s:%s&#34; % (k, v) for k, v in df.dtypes.items()])
        for col, t in t.items():
            if t == &#34;int64&#34;:
                rs.add(StructField(col, types.LongType()))
            elif t == &#34;int&#34;:
                rs.add(StructField(col, types.IntegerType()))
            elif t == &#34;float64&#34;:
                rs.add(StructField(col, types.FloatType()))
            elif t == &#34;bool&#34;:
                rs.add(StructField(col, types.BooleanType()))
            elif t == &#34;datetime64&#34;:
                rs.add(StructField(col, types.DateType()))
            else:
                rs.add(StructField(col, types.StringType()))
        return rs

    @staticmethod
    @timeit
    def import_batch(data, session, schema):
        if schema is not None:
            df_temp = session.createDataFrame(data, schema)
        else:
            df_temp = session.createDataFrame(data)
        return df_temp

    @staticmethod
    def from_iterator(iter,
                      batch_size=DEFAULT_BATCH_SIZE,
                      transform_item=None,
                      spark_session=None,
                      schema=None,
                      auto_correct_schema=False,
                      **kwargs):
        &#34;&#34;&#34;
        Create dataframe with list of data: [{&#34;id&#34; : 1,&#34;name&#34;: &#34;phong&#34;},{&#34;id&#34; : 2,&#34;name&#34;: &#34;phong&#34;}]

            iter: list data
            batch_size: data will split into batches, this is config of size of each batch
            transform_item: transform function if you want to transform each value
            spark_session:
            schema: Spark Schema DF of Spark
            kwargs:
            return:
        
        &#34;&#34;&#34;
        from zuka_etl.custom.spark_hook import SparkHook
        f = generate_batches(iter, batch_size_limit=batch_size, callback_item=transform_item, remove_none=True)
        sp = spark_session if spark_session is not None else SparkHook().session
        df = None
        n = 0
        total = 0
        for batch in f:
            if auto_correct_schema is True and n == 0 and schema is None:
                schema = SparkDfFromIterator.correct_dtype_by_pandas(data=batch)
                logger.info(&#34;schema auto correct is: %s&#34; % schema)
            df_temp = SparkDfFromIterator.import_batch(batch, sp, schema)
            if n &lt;= 0:
                df = df_temp
            else:
                df = df.union(df_temp)
            total += len(batch)
            logger.info(&#34;Import to df: %s done!&#34; % total)
            n += 1
        return df

    @staticmethod
    def from_iterator_with_checkpoint(iter, checkpoint_func, return_func, batch_size=DEFAULT_BATCH_SIZE,
                                      transform_item=None,
                                      spark_session=None,
                                      schema=None,
                                      auto_correct_schema=False,
                                      **kwargs):
        &#34;&#34;&#34;
        Create dataframe with list of data: [{&#34;id&#34; : 1,&#34;name&#34;: &#34;phong&#34;},{&#34;id&#34; : 2,&#34;name&#34;: &#34;phong&#34;}]

            iter: list data
            checkpoint_func: function process a dataframe. Format: checkpoint_func(index_batch, df_batch)
            return_func: function output data: Format: return_func(spark_session)
            batch_size: data will split into batches, this is config of size of each batch
            transform_item: transform function if you want to transform each value
            spark_session:
            schema: Spark Schema DF of Spark
            kwargs:
            return:
        
        &#34;&#34;&#34;
        from zuka_etl.custom.spark_hook import SparkHook
        if callable(checkpoint_func) is False:
            raise ValueError(&#34;checkpoint_func must be function&#34;)
        f = generate_batches(iter, batch_size_limit=batch_size, callback_item=transform_item, remove_none=True)
        sp = spark_session if spark_session is not None else SparkHook().session
        n = 0
        total = 0
        for batch in f:
            if auto_correct_schema is True and n == 0 and schema is None:
                schema = SparkDfFromIterator.correct_dtype_by_pandas(data=batch)
                logger.info(&#34;schema auto correct is: %s&#34; % schema)
            df_temp = SparkDfFromIterator.import_batch(batch, sp, schema)
            logger.info(&#34;Process checkpoint from index: %s&#34; % n)
            checkpoint_func(n, df_temp)
            total += len(batch)
            logger.info(&#34;Import to df: %s done!&#34; % total)
            n += 1

        return return_func(sp)

    @staticmethod
    def from_iterator_with_checkpoint_hdfs(iter,
                                           write_function,
                                           return_function,
                                           batch_size=DEFAULT_BATCH_SIZE,
                                           transform_item=None,
                                           spark_session=None,
                                           schema=None, mode=&#34;append&#34;,
                                           auto_correct_schema=False,
                                           **kwargs):
        &#34;&#34;&#34;
        Create dataframe with list of data: [{&#34;id&#34; : 1,&#34;name&#34;: &#34;phong&#34;},{&#34;id&#34; : 2,&#34;name&#34;: &#34;phong&#34;}]

            iter: list data
            write_function: callable function for writing data: write_function(p): p.format(&#34;parquet&#34;).save(path hdfs)
            return_function: function for output data: Format: return_function(spark_session)
            batch_size: data will split into batches, this is config of size of each batch
            transform_item: transform function if you want to transform each value
            spark_session:
            schema: Spark Schema DF of Spark
            mode: append or replace targer ouput
            kwargs:
            return:
        
        &#34;&#34;&#34;
        if callable(write_function) is False:
            raise ValueError(&#34;write_function must be function&#34;)

        def checkpoint_func(index, df):
            c = df.write.mode(&#34;overwrite&#34; if (index == 0 and mode == &#34;replace&#34;) else &#34;append&#34;)
            return write_function(c)

        return SparkDfFromIterator.from_iterator_with_checkpoint(iter=iter,
                                                                 checkpoint_func=checkpoint_func,
                                                                 return_func=return_function,
                                                                 batch_size=batch_size,
                                                                 transform_item=transform_item,
                                                                 spark_session=spark_session,
                                                                 schema=schema,
                                                                 auto_correct_schema=auto_correct_schema,
                                                                 **kwargs
                                                                 )</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.DEFAULT_BATCH_SIZE"><code class="name">var <span class="ident">DEFAULT_BATCH_SIZE</span></code></dt>
<dd>
<div class="desc"><p>Import data from iterator to Spark Dataframe</p></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.correct_dtype_by_pandas"><code class="name flex">
<span>def <span class="ident">correct_dtype_by_pandas</span></span>(<span>data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def correct_dtype_by_pandas(data):
    df = pd.DataFrame(data)
    t = df.dtypes
    rs = StructType()
    logger.info(&#34;Pandas type: %s&#34; % [&#34;%s:%s&#34; % (k, v) for k, v in df.dtypes.items()])
    for col, t in t.items():
        if t == &#34;int64&#34;:
            rs.add(StructField(col, types.LongType()))
        elif t == &#34;int&#34;:
            rs.add(StructField(col, types.IntegerType()))
        elif t == &#34;float64&#34;:
            rs.add(StructField(col, types.FloatType()))
        elif t == &#34;bool&#34;:
            rs.add(StructField(col, types.BooleanType()))
        elif t == &#34;datetime64&#34;:
            rs.add(StructField(col, types.DateType()))
        else:
            rs.add(StructField(col, types.StringType()))
    return rs</code></pre>
</details>
</dd>
<dt id="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.from_iterator"><code class="name flex">
<span>def <span class="ident">from_iterator</span></span>(<span>iter, batch_size=100000, transform_item=None, spark_session=None, schema=None, auto_correct_schema=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Create dataframe with list of data: [{"id" : 1,"name": "phong"},{"id" : 2,"name": "phong"}]</p>
<pre><code>iter: list data
batch_size: data will split into batches, this is config of size of each batch
transform_item: transform function if you want to transform each value
spark_session:
schema: Spark Schema DF of Spark
kwargs:
return:
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_iterator(iter,
                  batch_size=DEFAULT_BATCH_SIZE,
                  transform_item=None,
                  spark_session=None,
                  schema=None,
                  auto_correct_schema=False,
                  **kwargs):
    &#34;&#34;&#34;
    Create dataframe with list of data: [{&#34;id&#34; : 1,&#34;name&#34;: &#34;phong&#34;},{&#34;id&#34; : 2,&#34;name&#34;: &#34;phong&#34;}]

        iter: list data
        batch_size: data will split into batches, this is config of size of each batch
        transform_item: transform function if you want to transform each value
        spark_session:
        schema: Spark Schema DF of Spark
        kwargs:
        return:
    
    &#34;&#34;&#34;
    from zuka_etl.custom.spark_hook import SparkHook
    f = generate_batches(iter, batch_size_limit=batch_size, callback_item=transform_item, remove_none=True)
    sp = spark_session if spark_session is not None else SparkHook().session
    df = None
    n = 0
    total = 0
    for batch in f:
        if auto_correct_schema is True and n == 0 and schema is None:
            schema = SparkDfFromIterator.correct_dtype_by_pandas(data=batch)
            logger.info(&#34;schema auto correct is: %s&#34; % schema)
        df_temp = SparkDfFromIterator.import_batch(batch, sp, schema)
        if n &lt;= 0:
            df = df_temp
        else:
            df = df.union(df_temp)
        total += len(batch)
        logger.info(&#34;Import to df: %s done!&#34; % total)
        n += 1
    return df</code></pre>
</details>
</dd>
<dt id="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.from_iterator_with_checkpoint"><code class="name flex">
<span>def <span class="ident">from_iterator_with_checkpoint</span></span>(<span>iter, checkpoint_func, return_func, batch_size=100000, transform_item=None, spark_session=None, schema=None, auto_correct_schema=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Create dataframe with list of data: [{"id" : 1,"name": "phong"},{"id" : 2,"name": "phong"}]</p>
<pre><code>iter: list data
checkpoint_func: function process a dataframe. Format: checkpoint_func(index_batch, df_batch)
return_func: function output data: Format: return_func(spark_session)
batch_size: data will split into batches, this is config of size of each batch
transform_item: transform function if you want to transform each value
spark_session:
schema: Spark Schema DF of Spark
kwargs:
return:
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_iterator_with_checkpoint(iter, checkpoint_func, return_func, batch_size=DEFAULT_BATCH_SIZE,
                                  transform_item=None,
                                  spark_session=None,
                                  schema=None,
                                  auto_correct_schema=False,
                                  **kwargs):
    &#34;&#34;&#34;
    Create dataframe with list of data: [{&#34;id&#34; : 1,&#34;name&#34;: &#34;phong&#34;},{&#34;id&#34; : 2,&#34;name&#34;: &#34;phong&#34;}]

        iter: list data
        checkpoint_func: function process a dataframe. Format: checkpoint_func(index_batch, df_batch)
        return_func: function output data: Format: return_func(spark_session)
        batch_size: data will split into batches, this is config of size of each batch
        transform_item: transform function if you want to transform each value
        spark_session:
        schema: Spark Schema DF of Spark
        kwargs:
        return:
    
    &#34;&#34;&#34;
    from zuka_etl.custom.spark_hook import SparkHook
    if callable(checkpoint_func) is False:
        raise ValueError(&#34;checkpoint_func must be function&#34;)
    f = generate_batches(iter, batch_size_limit=batch_size, callback_item=transform_item, remove_none=True)
    sp = spark_session if spark_session is not None else SparkHook().session
    n = 0
    total = 0
    for batch in f:
        if auto_correct_schema is True and n == 0 and schema is None:
            schema = SparkDfFromIterator.correct_dtype_by_pandas(data=batch)
            logger.info(&#34;schema auto correct is: %s&#34; % schema)
        df_temp = SparkDfFromIterator.import_batch(batch, sp, schema)
        logger.info(&#34;Process checkpoint from index: %s&#34; % n)
        checkpoint_func(n, df_temp)
        total += len(batch)
        logger.info(&#34;Import to df: %s done!&#34; % total)
        n += 1

    return return_func(sp)</code></pre>
</details>
</dd>
<dt id="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.from_iterator_with_checkpoint_hdfs"><code class="name flex">
<span>def <span class="ident">from_iterator_with_checkpoint_hdfs</span></span>(<span>iter, write_function, return_function, batch_size=100000, transform_item=None, spark_session=None, schema=None, mode='append', auto_correct_schema=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Create dataframe with list of data: [{"id" : 1,"name": "phong"},{"id" : 2,"name": "phong"}]</p>
<pre><code>iter: list data
write_function: callable function for writing data: write_function(p): p.format("parquet").save(path hdfs)
return_function: function for output data: Format: return_function(spark_session)
batch_size: data will split into batches, this is config of size of each batch
transform_item: transform function if you want to transform each value
spark_session:
schema: Spark Schema DF of Spark
mode: append or replace targer ouput
kwargs:
return:
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_iterator_with_checkpoint_hdfs(iter,
                                       write_function,
                                       return_function,
                                       batch_size=DEFAULT_BATCH_SIZE,
                                       transform_item=None,
                                       spark_session=None,
                                       schema=None, mode=&#34;append&#34;,
                                       auto_correct_schema=False,
                                       **kwargs):
    &#34;&#34;&#34;
    Create dataframe with list of data: [{&#34;id&#34; : 1,&#34;name&#34;: &#34;phong&#34;},{&#34;id&#34; : 2,&#34;name&#34;: &#34;phong&#34;}]

        iter: list data
        write_function: callable function for writing data: write_function(p): p.format(&#34;parquet&#34;).save(path hdfs)
        return_function: function for output data: Format: return_function(spark_session)
        batch_size: data will split into batches, this is config of size of each batch
        transform_item: transform function if you want to transform each value
        spark_session:
        schema: Spark Schema DF of Spark
        mode: append or replace targer ouput
        kwargs:
        return:
    
    &#34;&#34;&#34;
    if callable(write_function) is False:
        raise ValueError(&#34;write_function must be function&#34;)

    def checkpoint_func(index, df):
        c = df.write.mode(&#34;overwrite&#34; if (index == 0 and mode == &#34;replace&#34;) else &#34;append&#34;)
        return write_function(c)

    return SparkDfFromIterator.from_iterator_with_checkpoint(iter=iter,
                                                             checkpoint_func=checkpoint_func,
                                                             return_func=return_function,
                                                             batch_size=batch_size,
                                                             transform_item=transform_item,
                                                             spark_session=spark_session,
                                                             schema=schema,
                                                             auto_correct_schema=auto_correct_schema,
                                                             **kwargs
                                                             )</code></pre>
</details>
</dd>
<dt id="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.import_batch"><code class="name flex">
<span>def <span class="ident">import_batch</span></span>(<span>*args, **kw)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def timed(*args, **kw):
    ts = time.time()
    result = method(*args, **kw)
    te = time.time()
    logger.info(&#34;[PROCESS TIME]: &#34; + (&#39;%r (%r, %r) %2.2f sec&#39; % \
                                      (method.__name__, type(args), type(kw), te - ts)))
    return result</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="zuka_etl.pipeline.extract" href="index.html">zuka_etl.pipeline.extract</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver" href="#zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver">SparkDfFromDriver</a></code></h4>
<ul class="">
<li><code><a title="zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver.cast_columns" href="#zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver.cast_columns">cast_columns</a></code></li>
<li><code><a title="zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver.from_csv" href="#zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver.from_csv">from_csv</a></code></li>
<li><code><a title="zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver.from_elastic" href="#zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver.from_elastic">from_elastic</a></code></li>
<li><code><a title="zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver.from_jdbc" href="#zuka_etl.pipeline.extract.spark_utils.SparkDfFromDriver.from_jdbc">from_jdbc</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator" href="#zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator">SparkDfFromIterator</a></code></h4>
<ul class="">
<li><code><a title="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.DEFAULT_BATCH_SIZE" href="#zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.DEFAULT_BATCH_SIZE">DEFAULT_BATCH_SIZE</a></code></li>
<li><code><a title="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.correct_dtype_by_pandas" href="#zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.correct_dtype_by_pandas">correct_dtype_by_pandas</a></code></li>
<li><code><a title="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.from_iterator" href="#zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.from_iterator">from_iterator</a></code></li>
<li><code><a title="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.from_iterator_with_checkpoint" href="#zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.from_iterator_with_checkpoint">from_iterator_with_checkpoint</a></code></li>
<li><code><a title="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.from_iterator_with_checkpoint_hdfs" href="#zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.from_iterator_with_checkpoint_hdfs">from_iterator_with_checkpoint_hdfs</a></code></li>
<li><code><a title="zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.import_batch" href="#zuka_etl.pipeline.extract.spark_utils.SparkDfFromIterator.import_batch">import_batch</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.3</a>.</p>
</footer>
</body>
</html>