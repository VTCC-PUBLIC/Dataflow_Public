<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.3" />
<title>zuka_etl.contribution.etl_full_flow.operators API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>zuka_etl.contribution.etl_full_flow.operators</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from airflow.operators.python_operator import PythonOperator
from airflow.utils.decorators import apply_defaults

from zuka_etl.contribution.etl_full_flow.models import LogEtlMetadata
from zuka_etl.contribution.etl_full_flow.process import SQLToAnyFlowBySpark, SQLToAnyFlowByPandas, AnyToAnyFlow
from zuka_etl.custom.spark_hook import SparkHook
from zuka_etl.helpers.time_utils import convert_unixtime, current_time_local, current_unixtime, convert_from_unixtime
from zuka_etl.log import logger

class SQLToAnyOperator(PythonOperator):
    @apply_defaults
    def __init__(
            self,
            sql_extract,  # type: any,
            from_connection_id: str,
            from_spark_conf: str = None,
            spark_conf: dict = {},
            transform=None,  # type: any
            load=None,  # type: Callable,
            complete=None,  # type: Callable,
            before_run=None,
            calculate_checkpoint_func=False,
            update_checkpoint_from_field: list = [&#34;change_time&#34;],
            get_last_checkpoint=False,
            log_metadata_model: LogEtlMetadata = None,
            save_metadata_log=False,
            engine: str = &#34;spark&#34;,
            start_time: int = None,
            end_time: int = None,
            sql_params: dict = {},
            additional_kwargs: dict = {},  # type: Optional[Dict]
            provide_context: bool = False,  # type: bool
            templates_dict=None,  # type: Optional[Dict]
            templates_exts=None,  # type: Optional[Iterable[str]]
            *args,
            **kwargs
    ):
        &#34;&#34;&#34;
            Operator ETL with input is SQL Query
            Using PySpark or Pandas as execution engine with JDBC connection
            Providing options for detect changing data with storing last change time on metadata:
            
                sql_extract:List or Str. Query SQL query that used for extract data
                from_connection_id: JDBC connection for data source, defined on Airflow
                from_spark_conf: spark config that is defined on Airflow
                spark_conf: list params of SparkHook
                transform: callable or list of SparkSQL (only for engine is spark)
                load: callable. Function that will be used for saving data
                complete: callable. Function will be triggered after ETL done
                calculate_checkpoint_func: None, &#39;auto&#39;, callable.
                update_checkpoint_from_field: List[Str]. List fields that will be calculated max(value) to save on
                                                                 metadata.
                                                                 That will be helpful for next job
                get_last_checkpoint: bool. if True, Process will automatically get maximum change_time of
                                                       successful jobs
                log_metadata_model: metadata model (sqlachemy model) to store metadata log
                save_metadata_log: bool. if True, Process will insert lo to metadata
                engine: str. spark or pandas
                start_time: int. Unixtime
                end_time: int. Unixtime
                sql_params: dict. params that will be used for replacing variable on sql query
                additional_kwargs:
                provide_context:
                templates_dict:
                templates_exts:
                args:
                kwargs:

            Ex:
            
                define a task
               
                    t1 = SQLToAnyOperator(
                        task_id=&#34;sync_dim_stock&#34;,
                        from_connection_id=&#34;jdbc_data_source&#34;,
                        sql_extract=&#39;select * from A where created_date &gt;= {start_time} and type = &#34;{test}&#34;&#39;, // start_time will be automatically calculated
                        sql_params={&#34;test&#34;: 1},
                        load=lambda df: SparkDfToDriver.to_hive(table=&#34;test&#34;, mode=SparkDfToDriver.MODE_OVERWRITE,
                                                                        spark_df=df,
                                                                        cast_columns={
                                                                            &#34;imported_time&#34;: &#34;int&#34;,
                                                                        }),
                        dag=dags
                        )
        &#34;&#34;&#34;
        super(SQLToAnyOperator, self).__init__(
            python_callable=lambda _: None,
            op_args=None,
            op_kwargs=None,
            provide_context=provide_context,
            templates_dict=templates_dict,
            templates_exts=templates_exts, *args, **kwargs)

        dag_start_unixtime = self.dag.start_date.timestamp() if self.dag.start_date else current_unixtime()
        dag_end_unixtime = self.dag.end_date.timestamp() if self.dag.end_date else convert_unixtime(
            current_time_local(end_day=True))
        dag_start_time = convert_from_unixtime(dag_start_unixtime)
        dag_end_time = convert_from_unixtime(dag_end_unixtime)
        sql_params.update({
            &#34;dag_start_time&#34;: dag_start_time.strftime(&#34;%Y-%m-%d %H:%M:%S.%f&#34;),
            &#34;dag_end_time&#34;: dag_end_time.strftime(&#34;%Y-%m-%d %H:%M:%S.%f&#34;),
            &#34;dag_start_date&#34;: dag_start_time.strftime(&#34;%Y-%m-%d&#34;),
            &#34;dag_end_date&#34;: dag_end_time.strftime(&#34;%Y-%m-%d&#34;),
            &#34;dag_start_date_dim&#34;: int(dag_start_time.strftime(&#34;%Y%m%d&#34;)),
            &#34;dag_end_date_dim&#34;: int(dag_end_time.strftime(&#34;%Y%m%d&#34;)),
            &#34;dag_start_unixtime&#34;: dag_start_unixtime,
            &#34;dag_end_unixtime&#34;: dag_end_unixtime
        })

        if engine == &#34;spark&#34;:
            self.python_callable = SQLToAnyFlowBySpark(task_id=self.task_id,
                                                       dag_id=self.dag_id,
                                                       sql_extract=sql_extract,
                                                       from_connection_id=from_connection_id,
                                                       from_spark_conf=from_spark_conf,
                                                       spark_conf=spark_conf,
                                                       transform=transform,
                                                       load=load,
                                                       complete=complete,
                                                       calculate_checkpoint_func=calculate_checkpoint_func,
                                                       get_last_checkpoint=get_last_checkpoint,
                                                       save_metadata_log=save_metadata_log,
                                                       start_time=start_time,
                                                       end_time=end_time,
                                                       sql_params=sql_params,
                                                       additional_kwargs=additional_kwargs,
                                                       log_metadata_model=log_metadata_model,
                                                       update_checkpoint_from_field=update_checkpoint_from_field,
                                                       before_run=before_run
                                                       ).run
        elif engine == &#34;pandas&#34;:
            self.python_callable = SQLToAnyFlowByPandas(task_id=self.task_id,
                                                        dag_id=self.dag_id,
                                                        sql_extract=sql_extract,
                                                        from_connection_id=from_connection_id,
                                                        transform=transform,
                                                        load=load,
                                                        complete=complete,
                                                        calculate_checkpoint_func=calculate_checkpoint_func,
                                                        get_last_checkpoint=get_last_checkpoint,
                                                        start_time=start_time,
                                                        end_time=end_time,
                                                        additional_kwargs=additional_kwargs,
                                                        log_metadata_model=log_metadata_model,
                                                        update_checkpoint_from_field=update_checkpoint_from_field,
                                                        before_run=before_run
                                                        ).run
        else:
            raise ValueError(&#34;engine: %s is invalid&#34; % engine)

    def execute(self, context):
        super(SQLToAnyOperator, self).execute(context=context)


class AnyToAnyOperator(PythonOperator):
    @apply_defaults
    def __init__(
            self,
            extract,  # type: callable,
            transform=None,  # type: Callable
            load=None,  # type: Callable,
            complete=None,  # type: Callable,
            calculate_checkpoint_func=False,
            update_checkpoint_from_field=[&#34;change_time&#34;],
            get_last_checkpoint=False,
            save_metadata_log=False,
            log_metadata_model=False,
            start_time=None,
            end_time=None,
            more_params={},  # type:  Optional[Dict]
            additional_kwargs={},  # type: Optional[Dict]
            provide_context=False,  # type: bool
            templates_dict=None,  # type: Optional[Dict]
            templates_exts=None,  # type: Optional[Iterable[str]]
            *args,
            **kwargs
    ):
        super(AnyToAnyOperator, self).__init__(
            python_callable=lambda _: None,
            op_args=None,
            op_kwargs=None,
            provide_context=provide_context,
            templates_dict=templates_dict,
            templates_exts=templates_exts, *args, **kwargs)
        # parsing start_date and end_date from dags

        dag_start_unixtime = self.dag.start_date.timestamp() if self.dag.start_date else current_unixtime()
        dag_end_unixtime = self.dag.end_date.timestamp() if self.dag.end_date else convert_unixtime(
            current_time_local(end_day=True))
        dag_start_time = convert_from_unixtime(dag_start_unixtime)
        dag_end_time = convert_from_unixtime(dag_end_unixtime)
        more_params.update({
            &#34;dag_start_time&#34;: dag_start_time.strftime(&#34;%Y-%m-%d %H:%M:%S.%f&#34;),
            &#34;dag_end_time&#34;: dag_end_time.strftime(&#34;%Y-%m-%d %H:%M:%S.%f&#34;),
            &#34;dag_start_date&#34;: dag_start_time.strftime(&#34;%Y-%m-%d&#34;),
            &#34;dag_end_date&#34;: dag_end_time.strftime(&#34;%Y-%m-%d&#34;),
            &#34;dag_start_date_dim&#34;: int(dag_start_time.strftime(&#34;%Y%m%d&#34;)),
            &#34;dag_end_date_dim&#34;: int(dag_end_time.strftime(&#34;%Y%m%d&#34;)),
            &#34;dag_start_unixtime&#34;: dag_start_unixtime,
            &#34;dag_end_unixtime&#34;: dag_end_unixtime
        })
        self.python_callable_object = AnyToAnyFlow(task_id=self.task_id,
                                                   dag_id=self.dag_id,
                                                   extract=extract,
                                                   transform=transform,
                                                   load=load,
                                                   complete=complete,
                                                   calculate_checkpoint_func=calculate_checkpoint_func,
                                                   get_last_checkpoint=get_last_checkpoint,
                                                   save_metadata_log=save_metadata_log,
                                                   start_time=start_time,
                                                   end_time=end_time,
                                                   more_params=more_params,
                                                   additional_kwargs=additional_kwargs,
                                                   log_metadata_model=log_metadata_model,
                                                   update_checkpoint_from_field=update_checkpoint_from_field
                                                   )

    def execute(self, context):
        self.python_callable_object.dag_context = context
        try:
            key_from_ct = [&#39;ds&#39;, &#39;next_ds&#39;, &#39;prev_ds&#39;, &#39;ds_nodash&#39;, &#39;yesterday_ds&#39;, &#39;tomorrow_ds&#39;,
                           &#39;end_date&#39;, &#39;latest_date&#39;]
            build_params = {(&#39;dag_ctx_%s&#39; % k): context.get(k, &#39;&#39;) for k in key_from_ct}
            execution_date = context[&#39;execution_date&#39;].timestamp()
            next_execution_date = context[&#39;next_execution_date&#39;].timestamp()
            build_params.update({
                &#39;dag_ctx_execution_time&#39;: convert_from_unixtime(execution_date).strftime(&#34;%Y-%m-%d %H:%M:%S&#34;),
                &#39;dag_ctx_execution_date&#39;: convert_from_unixtime(execution_date).strftime(&#34;%Y-%m-%d&#34;),
                &#39;dag_ctx_execution_dim_date&#39;: int(convert_from_unixtime(execution_date).strftime(&#34;%Y%m%d&#34;)),
                &#39;dag_ctx_next_execution_time&#39;: convert_from_unixtime(next_execution_date).strftime(&#34;%Y-%m-%d %H:%M:%S&#34;),
                &#39;dag_ctx_next_execution_date&#39;: convert_from_unixtime(next_execution_date).strftime(&#34;%Y-%m-%d&#34;),
                &#39;dag_ctx_next_execution_dim_date&#39;: int(convert_from_unixtime(next_execution_date).strftime(&#34;%Y%m%d&#34;))
            })
        except BaseException as e:
            logger.error(&#34;Error when set some default values from ariflow_context: %s.\nSkip setting these values...&#34; % e)
        self.python_callable_object.params.update(build_params)
        self.python_callable = self.python_callable_object.run
        super(AnyToAnyOperator, self).execute(context=context)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="zuka_etl.contribution.etl_full_flow.operators.AnyToAnyOperator"><code class="flex name class">
<span>class <span class="ident">AnyToAnyOperator</span></span>
<span>(</span><span>extract, transform=None, load=None, complete=None, calculate_checkpoint_func=False, update_checkpoint_from_field=['change_time'], get_last_checkpoint=False, save_metadata_log=False, log_metadata_model=False, start_time=None, end_time=None, more_params={}, additional_kwargs={}, provide_context=False, templates_dict=None, templates_exts=None, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Executes a Python callable</p>
<div class="admonition seealso">
<p class="admonition-title">Seealso</p>
<p>For more information on how to use this operator, take a look at the guide:
:ref:<code>howto/operator:PythonOperator</code></p>
</div>
<p>:param python_callable: A reference to an object that is callable
:type python_callable: python callable
:param op_kwargs: a dictionary of keyword arguments that will get unpacked
in your function
:type op_kwargs: dict (templated)
:param op_args: a list of positional arguments that will get unpacked when
calling your callable
:type op_args: list (templated)
:param provide_context: if set to true, Airflow will pass a set of
keyword arguments that can be used in your function. This set of
kwargs correspond exactly to what you can use in your jinja
templates. For this to work, you need to define <code>**kwargs</code> in your
function header.
:type provide_context: bool
:param templates_dict: a dictionary where the values are templates that
will get templated by the Airflow engine sometime between
<code>__init__</code> and <code>execute</code> takes place and are made available
in your callable's context after the template has been applied. (templated)
:type templates_dict: dict[str]
:param templates_exts: a list of file extensions to resolve while
processing templated fields, for examples <code>['.sql', '.hql']</code>
:type templates_exts: list[str]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AnyToAnyOperator(PythonOperator):
    @apply_defaults
    def __init__(
            self,
            extract,  # type: callable,
            transform=None,  # type: Callable
            load=None,  # type: Callable,
            complete=None,  # type: Callable,
            calculate_checkpoint_func=False,
            update_checkpoint_from_field=[&#34;change_time&#34;],
            get_last_checkpoint=False,
            save_metadata_log=False,
            log_metadata_model=False,
            start_time=None,
            end_time=None,
            more_params={},  # type:  Optional[Dict]
            additional_kwargs={},  # type: Optional[Dict]
            provide_context=False,  # type: bool
            templates_dict=None,  # type: Optional[Dict]
            templates_exts=None,  # type: Optional[Iterable[str]]
            *args,
            **kwargs
    ):
        super(AnyToAnyOperator, self).__init__(
            python_callable=lambda _: None,
            op_args=None,
            op_kwargs=None,
            provide_context=provide_context,
            templates_dict=templates_dict,
            templates_exts=templates_exts, *args, **kwargs)
        # parsing start_date and end_date from dags

        dag_start_unixtime = self.dag.start_date.timestamp() if self.dag.start_date else current_unixtime()
        dag_end_unixtime = self.dag.end_date.timestamp() if self.dag.end_date else convert_unixtime(
            current_time_local(end_day=True))
        dag_start_time = convert_from_unixtime(dag_start_unixtime)
        dag_end_time = convert_from_unixtime(dag_end_unixtime)
        more_params.update({
            &#34;dag_start_time&#34;: dag_start_time.strftime(&#34;%Y-%m-%d %H:%M:%S.%f&#34;),
            &#34;dag_end_time&#34;: dag_end_time.strftime(&#34;%Y-%m-%d %H:%M:%S.%f&#34;),
            &#34;dag_start_date&#34;: dag_start_time.strftime(&#34;%Y-%m-%d&#34;),
            &#34;dag_end_date&#34;: dag_end_time.strftime(&#34;%Y-%m-%d&#34;),
            &#34;dag_start_date_dim&#34;: int(dag_start_time.strftime(&#34;%Y%m%d&#34;)),
            &#34;dag_end_date_dim&#34;: int(dag_end_time.strftime(&#34;%Y%m%d&#34;)),
            &#34;dag_start_unixtime&#34;: dag_start_unixtime,
            &#34;dag_end_unixtime&#34;: dag_end_unixtime
        })
        self.python_callable_object = AnyToAnyFlow(task_id=self.task_id,
                                                   dag_id=self.dag_id,
                                                   extract=extract,
                                                   transform=transform,
                                                   load=load,
                                                   complete=complete,
                                                   calculate_checkpoint_func=calculate_checkpoint_func,
                                                   get_last_checkpoint=get_last_checkpoint,
                                                   save_metadata_log=save_metadata_log,
                                                   start_time=start_time,
                                                   end_time=end_time,
                                                   more_params=more_params,
                                                   additional_kwargs=additional_kwargs,
                                                   log_metadata_model=log_metadata_model,
                                                   update_checkpoint_from_field=update_checkpoint_from_field
                                                   )

    def execute(self, context):
        self.python_callable_object.dag_context = context
        try:
            key_from_ct = [&#39;ds&#39;, &#39;next_ds&#39;, &#39;prev_ds&#39;, &#39;ds_nodash&#39;, &#39;yesterday_ds&#39;, &#39;tomorrow_ds&#39;,
                           &#39;end_date&#39;, &#39;latest_date&#39;]
            build_params = {(&#39;dag_ctx_%s&#39; % k): context.get(k, &#39;&#39;) for k in key_from_ct}
            execution_date = context[&#39;execution_date&#39;].timestamp()
            next_execution_date = context[&#39;next_execution_date&#39;].timestamp()
            build_params.update({
                &#39;dag_ctx_execution_time&#39;: convert_from_unixtime(execution_date).strftime(&#34;%Y-%m-%d %H:%M:%S&#34;),
                &#39;dag_ctx_execution_date&#39;: convert_from_unixtime(execution_date).strftime(&#34;%Y-%m-%d&#34;),
                &#39;dag_ctx_execution_dim_date&#39;: int(convert_from_unixtime(execution_date).strftime(&#34;%Y%m%d&#34;)),
                &#39;dag_ctx_next_execution_time&#39;: convert_from_unixtime(next_execution_date).strftime(&#34;%Y-%m-%d %H:%M:%S&#34;),
                &#39;dag_ctx_next_execution_date&#39;: convert_from_unixtime(next_execution_date).strftime(&#34;%Y-%m-%d&#34;),
                &#39;dag_ctx_next_execution_dim_date&#39;: int(convert_from_unixtime(next_execution_date).strftime(&#34;%Y%m%d&#34;))
            })
        except BaseException as e:
            logger.error(&#34;Error when set some default values from ariflow_context: %s.\nSkip setting these values...&#34; % e)
        self.python_callable_object.params.update(build_params)
        self.python_callable = self.python_callable_object.run
        super(AnyToAnyOperator, self).execute(context=context)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>airflow.operators.python_operator.PythonOperator</li>
<li>airflow.models.baseoperator.BaseOperator</li>
<li>airflow.utils.log.logging_mixin.LoggingMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="zuka_etl.contribution.etl_full_flow.operators.AnyToAnyOperator.execute"><code class="name flex">
<span>def <span class="ident">execute</span></span>(<span>self, context)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the main method to derive when creating an operator.
Context is the same dictionary used as when rendering jinja templates.</p>
<p>Refer to get_template_context for more context.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def execute(self, context):
    self.python_callable_object.dag_context = context
    try:
        key_from_ct = [&#39;ds&#39;, &#39;next_ds&#39;, &#39;prev_ds&#39;, &#39;ds_nodash&#39;, &#39;yesterday_ds&#39;, &#39;tomorrow_ds&#39;,
                       &#39;end_date&#39;, &#39;latest_date&#39;]
        build_params = {(&#39;dag_ctx_%s&#39; % k): context.get(k, &#39;&#39;) for k in key_from_ct}
        execution_date = context[&#39;execution_date&#39;].timestamp()
        next_execution_date = context[&#39;next_execution_date&#39;].timestamp()
        build_params.update({
            &#39;dag_ctx_execution_time&#39;: convert_from_unixtime(execution_date).strftime(&#34;%Y-%m-%d %H:%M:%S&#34;),
            &#39;dag_ctx_execution_date&#39;: convert_from_unixtime(execution_date).strftime(&#34;%Y-%m-%d&#34;),
            &#39;dag_ctx_execution_dim_date&#39;: int(convert_from_unixtime(execution_date).strftime(&#34;%Y%m%d&#34;)),
            &#39;dag_ctx_next_execution_time&#39;: convert_from_unixtime(next_execution_date).strftime(&#34;%Y-%m-%d %H:%M:%S&#34;),
            &#39;dag_ctx_next_execution_date&#39;: convert_from_unixtime(next_execution_date).strftime(&#34;%Y-%m-%d&#34;),
            &#39;dag_ctx_next_execution_dim_date&#39;: int(convert_from_unixtime(next_execution_date).strftime(&#34;%Y%m%d&#34;))
        })
    except BaseException as e:
        logger.error(&#34;Error when set some default values from ariflow_context: %s.\nSkip setting these values...&#34; % e)
    self.python_callable_object.params.update(build_params)
    self.python_callable = self.python_callable_object.run
    super(AnyToAnyOperator, self).execute(context=context)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="zuka_etl.contribution.etl_full_flow.operators.SQLToAnyOperator"><code class="flex name class">
<span>class <span class="ident">SQLToAnyOperator</span></span>
<span>(</span><span>sql_extract, from_connection_id: str, from_spark_conf: str = None, spark_conf: dict = {}, transform=None, load=None, complete=None, before_run=None, calculate_checkpoint_func=False, update_checkpoint_from_field: list = ['change_time'], get_last_checkpoint=False, log_metadata_model: <a title="zuka_etl.contribution.etl_full_flow.models.LogEtlMetadata" href="models.html#zuka_etl.contribution.etl_full_flow.models.LogEtlMetadata">LogEtlMetadata</a> = None, save_metadata_log=False, engine: str = 'spark', start_time: int = None, end_time: int = None, sql_params: dict = {}, additional_kwargs: dict = {}, provide_context: bool = False, templates_dict=None, templates_exts=None, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Executes a Python callable</p>
<div class="admonition seealso">
<p class="admonition-title">Seealso</p>
<p>For more information on how to use this operator, take a look at the guide:
:ref:<code>howto/operator:PythonOperator</code></p>
</div>
<p>:param python_callable: A reference to an object that is callable
:type python_callable: python callable
:param op_kwargs: a dictionary of keyword arguments that will get unpacked
in your function
:type op_kwargs: dict (templated)
:param op_args: a list of positional arguments that will get unpacked when
calling your callable
:type op_args: list (templated)
:param provide_context: if set to true, Airflow will pass a set of
keyword arguments that can be used in your function. This set of
kwargs correspond exactly to what you can use in your jinja
templates. For this to work, you need to define <code>**kwargs</code> in your
function header.
:type provide_context: bool
:param templates_dict: a dictionary where the values are templates that
will get templated by the Airflow engine sometime between
<code>__init__</code> and <code>execute</code> takes place and are made available
in your callable's context after the template has been applied. (templated)
:type templates_dict: dict[str]
:param templates_exts: a list of file extensions to resolve while
processing templated fields, for examples <code>['.sql', '.hql']</code>
:type templates_exts: list[str]</p>
<p>Operator ETL with input is SQL Query
Using PySpark or Pandas as execution engine with JDBC connection
Providing options for detect changing data with storing last change time on metadata:</p>
<pre><code>sql_extract:List or Str. Query SQL query that used for extract data
from_connection_id: JDBC connection for data source, defined on Airflow
from_spark_conf: spark config that is defined on Airflow
spark_conf: list params of SparkHook
transform: callable or list of SparkSQL (only for engine is spark)
load: callable. Function that will be used for saving data
complete: callable. Function will be triggered after ETL done
calculate_checkpoint_func: None, 'auto', callable.
update_checkpoint_from_field: List[Str]. List fields that will be calculated max(value) to save on
                                                 metadata.
                                                 That will be helpful for next job
get_last_checkpoint: bool. if True, Process will automatically get maximum change_time of
                                       successful jobs
log_metadata_model: metadata model (sqlachemy model) to store metadata log
save_metadata_log: bool. if True, Process will insert lo to metadata
engine: str. spark or pandas
start_time: int. Unixtime
end_time: int. Unixtime
sql_params: dict. params that will be used for replacing variable on sql query
additional_kwargs:
provide_context:
templates_dict:
templates_exts:
args:
kwargs:
</code></pre>
<h2 id="ex">Ex</h2>
<p>define a task</p>
<pre><code>t1 = SQLToAnyOperator(
    task_id="sync_dim_stock",
    from_connection_id="jdbc_data_source",
    sql_extract='select * from A where created_date &gt;= {start_time} and type = "{test}"', // start_time will be automatically calculated
    sql_params={"test": 1},
    load=lambda df: SparkDfToDriver.to_hive(table="test", mode=SparkDfToDriver.MODE_OVERWRITE,
                                                    spark_df=df,
                                                    cast_columns={
                                                        "imported_time": "int",
                                                    }),
    dag=dags
    )
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SQLToAnyOperator(PythonOperator):
    @apply_defaults
    def __init__(
            self,
            sql_extract,  # type: any,
            from_connection_id: str,
            from_spark_conf: str = None,
            spark_conf: dict = {},
            transform=None,  # type: any
            load=None,  # type: Callable,
            complete=None,  # type: Callable,
            before_run=None,
            calculate_checkpoint_func=False,
            update_checkpoint_from_field: list = [&#34;change_time&#34;],
            get_last_checkpoint=False,
            log_metadata_model: LogEtlMetadata = None,
            save_metadata_log=False,
            engine: str = &#34;spark&#34;,
            start_time: int = None,
            end_time: int = None,
            sql_params: dict = {},
            additional_kwargs: dict = {},  # type: Optional[Dict]
            provide_context: bool = False,  # type: bool
            templates_dict=None,  # type: Optional[Dict]
            templates_exts=None,  # type: Optional[Iterable[str]]
            *args,
            **kwargs
    ):
        &#34;&#34;&#34;
            Operator ETL with input is SQL Query
            Using PySpark or Pandas as execution engine with JDBC connection
            Providing options for detect changing data with storing last change time on metadata:
            
                sql_extract:List or Str. Query SQL query that used for extract data
                from_connection_id: JDBC connection for data source, defined on Airflow
                from_spark_conf: spark config that is defined on Airflow
                spark_conf: list params of SparkHook
                transform: callable or list of SparkSQL (only for engine is spark)
                load: callable. Function that will be used for saving data
                complete: callable. Function will be triggered after ETL done
                calculate_checkpoint_func: None, &#39;auto&#39;, callable.
                update_checkpoint_from_field: List[Str]. List fields that will be calculated max(value) to save on
                                                                 metadata.
                                                                 That will be helpful for next job
                get_last_checkpoint: bool. if True, Process will automatically get maximum change_time of
                                                       successful jobs
                log_metadata_model: metadata model (sqlachemy model) to store metadata log
                save_metadata_log: bool. if True, Process will insert lo to metadata
                engine: str. spark or pandas
                start_time: int. Unixtime
                end_time: int. Unixtime
                sql_params: dict. params that will be used for replacing variable on sql query
                additional_kwargs:
                provide_context:
                templates_dict:
                templates_exts:
                args:
                kwargs:

            Ex:
            
                define a task
               
                    t1 = SQLToAnyOperator(
                        task_id=&#34;sync_dim_stock&#34;,
                        from_connection_id=&#34;jdbc_data_source&#34;,
                        sql_extract=&#39;select * from A where created_date &gt;= {start_time} and type = &#34;{test}&#34;&#39;, // start_time will be automatically calculated
                        sql_params={&#34;test&#34;: 1},
                        load=lambda df: SparkDfToDriver.to_hive(table=&#34;test&#34;, mode=SparkDfToDriver.MODE_OVERWRITE,
                                                                        spark_df=df,
                                                                        cast_columns={
                                                                            &#34;imported_time&#34;: &#34;int&#34;,
                                                                        }),
                        dag=dags
                        )
        &#34;&#34;&#34;
        super(SQLToAnyOperator, self).__init__(
            python_callable=lambda _: None,
            op_args=None,
            op_kwargs=None,
            provide_context=provide_context,
            templates_dict=templates_dict,
            templates_exts=templates_exts, *args, **kwargs)

        dag_start_unixtime = self.dag.start_date.timestamp() if self.dag.start_date else current_unixtime()
        dag_end_unixtime = self.dag.end_date.timestamp() if self.dag.end_date else convert_unixtime(
            current_time_local(end_day=True))
        dag_start_time = convert_from_unixtime(dag_start_unixtime)
        dag_end_time = convert_from_unixtime(dag_end_unixtime)
        sql_params.update({
            &#34;dag_start_time&#34;: dag_start_time.strftime(&#34;%Y-%m-%d %H:%M:%S.%f&#34;),
            &#34;dag_end_time&#34;: dag_end_time.strftime(&#34;%Y-%m-%d %H:%M:%S.%f&#34;),
            &#34;dag_start_date&#34;: dag_start_time.strftime(&#34;%Y-%m-%d&#34;),
            &#34;dag_end_date&#34;: dag_end_time.strftime(&#34;%Y-%m-%d&#34;),
            &#34;dag_start_date_dim&#34;: int(dag_start_time.strftime(&#34;%Y%m%d&#34;)),
            &#34;dag_end_date_dim&#34;: int(dag_end_time.strftime(&#34;%Y%m%d&#34;)),
            &#34;dag_start_unixtime&#34;: dag_start_unixtime,
            &#34;dag_end_unixtime&#34;: dag_end_unixtime
        })

        if engine == &#34;spark&#34;:
            self.python_callable = SQLToAnyFlowBySpark(task_id=self.task_id,
                                                       dag_id=self.dag_id,
                                                       sql_extract=sql_extract,
                                                       from_connection_id=from_connection_id,
                                                       from_spark_conf=from_spark_conf,
                                                       spark_conf=spark_conf,
                                                       transform=transform,
                                                       load=load,
                                                       complete=complete,
                                                       calculate_checkpoint_func=calculate_checkpoint_func,
                                                       get_last_checkpoint=get_last_checkpoint,
                                                       save_metadata_log=save_metadata_log,
                                                       start_time=start_time,
                                                       end_time=end_time,
                                                       sql_params=sql_params,
                                                       additional_kwargs=additional_kwargs,
                                                       log_metadata_model=log_metadata_model,
                                                       update_checkpoint_from_field=update_checkpoint_from_field,
                                                       before_run=before_run
                                                       ).run
        elif engine == &#34;pandas&#34;:
            self.python_callable = SQLToAnyFlowByPandas(task_id=self.task_id,
                                                        dag_id=self.dag_id,
                                                        sql_extract=sql_extract,
                                                        from_connection_id=from_connection_id,
                                                        transform=transform,
                                                        load=load,
                                                        complete=complete,
                                                        calculate_checkpoint_func=calculate_checkpoint_func,
                                                        get_last_checkpoint=get_last_checkpoint,
                                                        start_time=start_time,
                                                        end_time=end_time,
                                                        additional_kwargs=additional_kwargs,
                                                        log_metadata_model=log_metadata_model,
                                                        update_checkpoint_from_field=update_checkpoint_from_field,
                                                        before_run=before_run
                                                        ).run
        else:
            raise ValueError(&#34;engine: %s is invalid&#34; % engine)

    def execute(self, context):
        super(SQLToAnyOperator, self).execute(context=context)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>airflow.operators.python_operator.PythonOperator</li>
<li>airflow.models.baseoperator.BaseOperator</li>
<li>airflow.utils.log.logging_mixin.LoggingMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="zuka_etl.contribution.etl_full_flow.operators.SQLToAnyOperator.execute"><code class="name flex">
<span>def <span class="ident">execute</span></span>(<span>self, context)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the main method to derive when creating an operator.
Context is the same dictionary used as when rendering jinja templates.</p>
<p>Refer to get_template_context for more context.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def execute(self, context):
    super(SQLToAnyOperator, self).execute(context=context)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="zuka_etl.contribution.etl_full_flow" href="index.html">zuka_etl.contribution.etl_full_flow</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="zuka_etl.contribution.etl_full_flow.operators.AnyToAnyOperator" href="#zuka_etl.contribution.etl_full_flow.operators.AnyToAnyOperator">AnyToAnyOperator</a></code></h4>
<ul class="">
<li><code><a title="zuka_etl.contribution.etl_full_flow.operators.AnyToAnyOperator.execute" href="#zuka_etl.contribution.etl_full_flow.operators.AnyToAnyOperator.execute">execute</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="zuka_etl.contribution.etl_full_flow.operators.SQLToAnyOperator" href="#zuka_etl.contribution.etl_full_flow.operators.SQLToAnyOperator">SQLToAnyOperator</a></code></h4>
<ul class="">
<li><code><a title="zuka_etl.contribution.etl_full_flow.operators.SQLToAnyOperator.execute" href="#zuka_etl.contribution.etl_full_flow.operators.SQLToAnyOperator.execute">execute</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.3</a>.</p>
</footer>
</body>
</html>